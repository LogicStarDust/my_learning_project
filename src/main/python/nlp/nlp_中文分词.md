# 词汇与分词技术

## 一、分词流派

1. 机械式分词基于词典，直接根据词典中的词条切分

    简单、实用，缺点是词典的完备性护问题。

2. 基于语法和规则的分词法

    进行语法、语义分析，利用句法信息和语义信息来进行词性标注，解决分词歧义。但是语法知识、句法规则十分复杂，所以基于语法和规则的分词法的精确度还不太好。
3. 基于统计的分词法

    根据字符串在语料库中出现的统计频率标记其是否是一个词。

## 二、词和分词规范

> 汉语信息处理使用的、具有确定的语义或语法功能的基本单位。  ——《信息处理用现代汉语分词规范》  

粗粒度和细粒度分词，细粒度不但要分词，还要对词内的语素进行切分。

文本中第$i$个词的出现与前面$i$-$n$个词相关。假设$W_1W_2W_3\cdots W_n$是长度为$n$的字符串,则词$W_i$的出现的似然度用方程表示为:
$$
\begin{aligned}
    P(W) &= P(W_1W_2\cdots W_n) \\\\
    &= P(W_1)P(W_2|W_1)P(W_3|W_1W_2)\cdots P(W_n|W_1W_2\cdots W_{n-1})\\\\
    &=\prod_{i=1}^n P(W_i|W_1W_2\cdots W_{i-1})
\end{aligned}
$$
为了简化计算，假设只与相邻的第一个词有关，即语言模型的二元模型。
$$
\begin{aligned}
    P(W)&\approx P(W_1)P(W_2|W_1)P(W_3|W_2)\cdots P(W_n|W_{n-1})\\\\
    &=\prod_{i=1}^n P(W_i|W_{i-1})
\end{aligned}
$$
所以当$n$=2时：
$$ \prod_{i=1}^n P(W_i|W_{i-1}) \approx \frac{Count(W_{i-1}|W_i)}{Count(W_{i-1})}$$
Count(.)表示一个特定的词序列在整个语料库中出现的累计次数。

### 词汇的构成与未登录词

未登录词识别（Named Entity Recognition,NER）,识别词汇因为能产性产生的新词。现在的方式有：

1. 构词算法角度识别
   基于构词规律识别，在专门领域如中国人名、译名、日本人民，效果较好。不同领域的识别不现实。
2. 半监督的条件随机场（semi_CRF）
   不同领域下，低成本，效果好

## 三、系统总体流程与词典结构

这里以HanLP实现为例。

1. 句子输入
2. 导入词典
3. 句子切分
4. 词汇粗分
    - 一元切分
    - 原子切分
    - 二元切分
    - N-最短路径
    - 应用规则
5. 未登录词识别
    - 人名识别
    - 译名识别
    - 地名识别
6. 优化细分逻辑同切分
7. 词性标注
8. 后处理
9. 输出

### 分词词典结构

- 一元模型词典
  - 格式为：“词   词性    词频    词性2   词频2   ······”
- 二元模型词典
  - 格式为：“前词@后词  共现频次”
  - 注意二元模型词典结合一元模型，其实二元模型本质上是一个行列都是一元模型中词的矩阵，或者通过二元模型的共现关系，构成一个以词为node,频次为边的图模型。
- 命名实体词典
  - 命名实体词典分两部分
    - 一元词典
      - 格式为：“词 元模式标签  词频    元模式标签2 词频2   ······”
      - 元模式标签：例如，B标签代表词为姓氏，E标签代表词的单名等
    - 二元词典
      - 由元模式标签作为行列，构成的二维矩阵，矩阵内容为对象行列的两个元模式标签组合的词频
    - 此外，还有多种元模式标签的组合串，比如BBE串代表“姓姓名”模式的人名。
  - 包括人名、译名、日本人名、地名、组织机构词典

### 词典的存储结构

加载字典到内存，先用TreeMap加载，然后build一个trie树,最终使用的是双数组Trie树。

1. trie树：节点关系为trie[i][j]=k
    - i:节点编号
    - j:i节点的第j个子节点
    - k:i节点的第j个子节点的编号

### 句子切分

直接匹配标点符号和换行符,切分成一个个句子

### 分词

1. 一元词网
    - 对句子，查询核心词典，原子切分确定类型，进行最大值匹配，把匹配出来的词和词的词性、词频，存储成二维数组。

2. 二元词图
    - 对一元词网，查询二元词典，最大值匹配，匹配结果形成一个Graph，即词图。
  
3. 输出最短路径
    - 针对二元词图（包含句子节点和根据字典算出来的边的权重），计算整个句子的最短路径，结果即是分词结果（可能选取两个），做成一个新词网。
  
4. 应用后处理规则
    - 一些对日期、数字等特殊处理的规则，并加入到新词网。
5. 细分和最短路径
    - 对加入了后处理规则的新词网，重新生成词图，并应用最短路径算法，计算出最后结果