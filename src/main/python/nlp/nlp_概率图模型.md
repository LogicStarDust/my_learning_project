# 概率图模型

## 一、概率论回顾

1. 基本概念（这里使用自然语言简述，严格的定义请用数学语言）
   - 联合概率
     - 多个随机变量，联合在一起的表示。（其实就是多个条件同时成立的概率，所有变量可能取值的组合的概率值和为1）
   - 边缘概率
     - 联合概率中，某一个（多个）随机变量，去除其他随机变量的概率。
   - 条件概率
     - 两个随机事件AB,A在B发生时候的发生概率，为P(A|B)，即条件概率。

2. 贝叶斯
   - 公式：
    $$
    P(X|Y)=\frac{P(Y|X)}{P(Y)}
    $$

3. 文本分类

## 二、信息熵

1. 信息量与信息熵

    信息量：
    $$
    I(X)=-\log P(X)
    $$
    信息熵：
    $$
    H(X)=\sum_x P(X)\log \frac{1}{P(X)}=-\sum_x P(X)log P(X)
    $$
        信息熵是对平均不确定性的度量，也是信息量的期望。假如随机变量X的可能性确定了，那么信息熵就是0。
2. 互信息  
消息经过信源发出后，会被干扰。假如发出的消息为x，干扰后为y，先验概率为p(x)，后验概率为p(x|y)。后验概率与先验概率的比值的对数为互信息量。  
互信息可理解为，收信者收到信息X后，对信源Y的不确定性的消除。

3. 联合熵  
    联合熵是借助联合概率分布对熵的自然推广，两个离散随机比变量X和Y的联合熵定义：
    $$
    H(X,Y)=\sum_{x,y}P(X,Y)\log\frac{1}{p(X,Y)}=-\sum_{XY}P(X,Y)logP(X,Y)
    $$
4. 条件熵  
    条件熵是利用条件概率分布对熵的一个延伸。如果知道随机变量Y取值为y,那么X的后延概率分榜急为P(x|Y=y),利用条件分布可以定义这时候的条件熵为：
    $$
    H(X|Y=y)=-\sum_xP(X|Y=y)log P(X|Y=y)
    $$
    熵的链式规则：
    $$
    H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y)\\
    I(X;Y)+H(X,Y)=H(X)+H(Y)
    $$

5. 交叉熵
   $$
   H(P;Q)=-\sum P_{p}(Z)logP_q(Z)
   $$
   是用一个非真实的概率分布衡量真实的概率分布时候得出的信息量期望。

6. KL散度(相对熵)
    $$
    KL(P,Q)=\sum_x P(X)\log \frac{P(X)}{Q(X)}
    $$
    是非真实的概率分布的交叉熵减去真实分布的信息熵

## 三、概率图模型